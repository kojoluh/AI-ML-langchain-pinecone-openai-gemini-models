{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1e39e-6b62-4f2b-bbfa-48a3e38e9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f413dc-9537-4dbc-9825-27592b237f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.')\n",
    "print(output.content)\n",
    "# help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00c968-a218-4cb8-a3d2-0ddff85d8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(message)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3485cb-8625-4ab4-b671-cea09cc70dea",
   "metadata": {},
   "source": [
    "### In-Memory Cache - Caching LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdeac04-de8f-4eb2-98e1-285992b588d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243834b-7dc3-48f9-a93a-ba5871c43ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa5ae5-898f-4560-884b-a7b9eeebf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f082d6-a856-48b0-92af-ad6015355842",
   "metadata": {},
   "source": [
    "### SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374eeb3-b5ff-43e1-9b70-f540df3b98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.sqlite.db\"))\n",
    "\n",
    "prompt2 = \"Tell me a joke\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5f78d-48a9-4840-bf14-6c1ca5853920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# first request (not in cache, takes longer)\n",
    "\n",
    "llm.invoke(prompt2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a8dd5-f1dc-465f-b4cd-d2f3a8bc25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# second request (cached, faster)\n",
    "llm.invoke(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82bbe8-d001-4624-a0d1-1d33bfbaacf1",
   "metadata": {},
   "source": [
    "## LLM Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67d50-e090-47f4-becc-f65340e0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f7cc0-0ecb-421b-b094-6f637d1bc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97ea07-b4cb-415e-9370-bb22da4319d2",
   "metadata": {},
   "source": [
    "# 20240301 - PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404d2d4-f609-458a-aa0c-0ef1987ad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in \"{language}\".'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language='english')\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe85cd-b2a8-4fef-addf-c2a9ff4074c4",
   "metadata": {},
   "source": [
    "# 20240301 - ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad4473-2a00-42ab-bf94-f6541f0e9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population. Show the population in millions.')\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(n='20', area='Africa')\n",
    "print(messages)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa1828-1198-4204-8a3d-17aa0d9ebb9e",
   "metadata": {},
   "source": [
    "20240301 - Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4dbb7-4d87-4b49-beea-50acb6b3c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in \"{language}\".'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'HSV', 'language': 'English'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b634ab8-e152-4b42-a453-d5799e14a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "output = chain.invoke(country)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aafbb5-0af6-4b35-b254-af73454b6c2b",
   "metadata": {},
   "source": [
    "# 20240301 - Sequential Chains\n",
    "With Sequential chains, you can make a series of calls to one or more LLMs. You can take the output from one chain and use it as the input to another chain.\n",
    "\n",
    "There are two types of sequential chains:\n",
    "1. SimpleSequentialChain\n",
    "2. General form of sequential chains\n",
    "\n",
    "SimpleSequentialChain represents a series of chains, where each individual chain has a single input and a single output, and the output of one step is used as input to the next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b19a1-f5b0-4872-886d-c920d6c4c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "     template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('linear regression')\n",
    "print(output['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658c0d-7ad7-4b6b-b56b-2f916a82a0db",
   "metadata": {},
   "source": [
    "# 20240301 - Agents - LangChain Agents in Action: Python REPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e35b0f-258c-42ee-b25b-dd25bafacd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c1b35-abc9-41c6-a26e-31f5d19fcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1932d-8847-479d-89c6-bffd1fe5d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88111d36-d8a9-4805-8d4c-1a123ef6ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "fact12 = math.factorial(12)\n",
    "sqrtFact12 = math.sqrt(fact12)\n",
    "roundSqrtFact12 = round(sqrtFact12, 4)\n",
    "\n",
    "print(fact12, sqrtFact12, roundSqrtFact12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c942b7-84fa-4349-8359-a921e4c037ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af243dd9-1ee8-4372-87d3-cde8d4cc34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6093efb-e189-44fe-b7cc-b6add506bfb4",
   "metadata": {},
   "source": [
    "# 20240301 - Langchain tools- DuckDuckGo and Wikipedia, Google Search\n",
    "Langchain tools are like specialized apps for yout LLM. They are tiny code modules that allow it to access information and services.\n",
    "\n",
    "These tools connect your LLM to search engines, databases, APIs, and more, expanding its knowledge and capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d6a2a-fe28-4b9a-8c28-8863faf8dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53127174-9735-4034-9e89-dda48843f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "output = search.invoke('Where was Adolf Hitler born?')\n",
    "print(output)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7c8ff-3a18-48c1-8eb9-28a9e698b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='en-GB', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Newport')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c48f8-ca0d-41d1-8121-34cfb5f2f2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87779cc3-0754-4907-82fd-4bd8daebbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = DuckDuckGoSearchResults()\n",
    "output2 = search2.run('Freddie Mercury and Queen')\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906932f9-84d7-4e69-b426-ba861b44d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229f0d-ca56-4b45-8af2-7db8295c3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a9745-0f75-41fd-a7be-b95ac29a210f",
   "metadata": {},
   "source": [
    "# Wikipedia tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1813d-8053-4fcb-b64b-ad2e509cf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a121fcd-0d19-4300-b530-fb915659a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=5000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.invoke({'query': 'llamaindex'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9c0ed-295c-460e-9e5d-4464a4ab01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.invoke('Great Wall of China')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d2528-4f2f-4748-831c-5b04f602c756",
   "metadata": {},
   "source": [
    "# 20240301 - Reasoning and Acting (ReAct)\n",
    "ReAct is a new approach that combines reasoning (chain of thoughts prompting) and acting capabilities of LLMs.\n",
    "\n",
    "With ReAct LLMs generate reasoning traces and task-specific actions in an interleaved manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c684b-c9fe-426e-9d54-8249d03ae8e2",
   "metadata": {},
   "source": [
    "# Creating a ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546c1b9-6fb0-496d-8cad-3746b48ce0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "template = '''\n",
    "Anser the following questions as best as you can. Translate to FRENCH.\n",
    "Questions: {q}\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "# print(type(prompt))\n",
    "# print(prompt.input_variables)\n",
    "# print(prompt.template)\n",
    "# 1. PythonREPL tool\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")\n",
    "\n",
    "# 2. Wikipedia Tool (for searchin Wikipedia)\n",
    "wiki_api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "ddg_search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=ddg_search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89108423-5ef5-4ea4-b8b3-63c19a535046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "# question = 'Who is the current Prime Minister of U.K.?'\n",
    "question = 'Tell me about Makaveli early life'\n",
    "\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd138e01-9c4b-400f-ab93-d22dd3cf1d69",
   "metadata": {},
   "source": [
    "# 20240302 - Embeddings\n",
    "Embeddings are the core of building LLMs applications. Text embeddings are numeric respresentations of text and are used in NLP and ML tasks.\n",
    "\n",
    "Embedings Applications\n",
    "----------------------\n",
    "Text classification: assigning a label to a piece of text.\n",
    "Text Clustering: grouping together pieces of text that are similar in meaning.\n",
    "Question-Answering: answering a question posed in natural language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a8a8-3154-4e99-9f99-62690ed24708",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "One of the biggest challenges of AI applications is efficient data processing\n",
    "Many of the latest AI applications rely on vector embeddings. Chatbots, question-answering systems, and machine translation rely on vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861f346-d61a-494d-b8d7-ba59a770382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project = Books Recommendation System\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Authentication\n",
    "def openai_authenticate(keyfile):\n",
    "    import openai\n",
    "    with open(keyfile, 'r') as f:\n",
    "        api_key = f.read().strip('\\n')\n",
    "        assert api_key.startsWith('sk-'), 'Error loading the API key. The API key starts with \"sk-\"'\n",
    "    openai.api_key = api_key\n",
    "\n",
    "openai_authenticate('../key.txt')\n",
    "\n",
    "\n",
    "# Loading the dataset into Pandas Dataframe\n",
    "df = pd.read_csv('./books_dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "df = df.sort_values('average_rating', ascending=False).head(2000)\n",
    "# df.iloc[97]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e04371-2cbb-496d-9291-79ec881a7cfe",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "vector databases are a new type of database, designed to store and query unstructured data.\n",
    "\n",
    "Unstructured data is data that does not have a fixed schema, such as text, images, and audio.\n",
    "\n",
    "SQL vs Vector databases\n",
    "Pipeline for vectir databases\n",
    "\n",
    "Vector databases use a combination fo different optimized algorithms that all participate in Approximate Nearest Neighbor (ANN) search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e829c26-3881-4568-acde-8630a63462af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258e051-a193-48c8-93b0-c022d7a1374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8312c5-ac0a-48b3-a824-2a4eb7e3876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade -q pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37110f1-6e77-4f73-bf1a-048ef54af4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa28bf-8246-4f5d-a019-fe8e724f6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone()\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b4b7d-1ae0-4b4c-b60d-2fbe46abdad8",
   "metadata": {},
   "source": [
    "# Pinecone Indexes\n",
    "An index is the highest level org unit of a vector data in Pinecone\n",
    "\n",
    "It accepts and stores vectors, serves queries over the vectors it contains, and does other vector operations over its contents.\n",
    "\n",
    "- Serverless indexes: you dont configure or manage any compute or storage resources (they scale automatically).\n",
    "- Pod-based indexes: you choose one or more preconfigured units of hardware (pods).\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c7a25-3c11-4551-ae3a-8a00572346ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.list_indexes()[0]\n",
    "# pc.describe_index('indexname')\n",
    "\n",
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfec58-89e6-4f41-9d18-aaf4434af810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import PodSpec\n",
    "index_name = 'langchain'\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f'Creating index {index_name}')\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=PodSpec(\n",
    "            environment='gcp-starter'\n",
    "        )\n",
    "    )\n",
    "    print('Index created! Yay!')\n",
    "else:\n",
    "    print(f'Index {index_name} already exists1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a448f2-8030-4f84-a22a-ba56df6d3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name in pc.list_indexes().names():\n",
    "    print(f'Deleting index {index_name}')\n",
    "    pc.delete_index(index_name)\n",
    "    print('Done')\n",
    "else:\n",
    "    print(f'Index {index_name} does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cb1d6-5daa-429f-86f3-6a22cfd3a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with index details + stats\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46cc22-a073-49a8-b6ca-661b138b15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inserting vectors\n",
    "import random\n",
    "\n",
    "vectors = [[random.random() for _ in range(1536)] for v in range(5)]\n",
    "# print(vectors)\n",
    "ids = list('abcde')\n",
    "\n",
    "index_name = 'langchain'\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert(vectors=zip(ids, vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6be96c-92e0-4e03-9702-bebdd493a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## updating vectors\n",
    "index.upsert(vectors=[('c', [0.5] * 1536)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5cdb10-3f9a-4212-a777-2ef659ecd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetching vectors\n",
    "# index = pc.Index(index_name)\n",
    "index.fetch(ids=['a', 'b', 'e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0af3d8-a9e5-4fa8-a3f0-99ae3734f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deleting vectors\n",
    "\n",
    "index.delete(ids=['b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e093cfa-d933-44b9-8507-3ff98c10e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c20f34-49e8-49bf-a662-c28a407ccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## query index\n",
    "query_vector = [random.random() for _ in range(1536)]\n",
    "\n",
    "index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=3,\n",
    "    include_values=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e6b47-3703-4c9b-a0d2-f52f2fa4d506",
   "metadata": {},
   "source": [
    "# Pinecome Namespaces\n",
    "- Pinecone allows you to partition the vectors in an index into namespaces.\n",
    "- Queries and other operations are scoped to a specific namespace, allowing different requests to search different subsets of your index.\n",
    "- Key information about namespaces:\n",
    "  - every index consists of one or more namespaces.\n",
    "  - each vector exists in exactly one namespace.\n",
    "  - namespaces are uniquely identified by a namespace name.\n",
    "  - the default namespace is represented by the empty string and is used if no specific namespace is specified.\n",
    "\n",
    "## Namespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36171ee3-3704-4a75-adfd-b9ccd92e6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.describe_index_stats()\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "import random\n",
    "vectors = [[random.random() for _ in range(1536)] for v in range(5)]\n",
    "# print(vectors)\n",
    "ids = list('abcde')\n",
    "\n",
    "index.upsert(vectors=zip(ids, vectors))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eff48-16ea-4973-8799-38c03c29ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[random.random() for _ in range(1536)] for v in range(3)]\n",
    "# print(vectors)\n",
    "ids = list('xyz')\n",
    "\n",
    "index.upsert(vectors=zip(ids, vectors), namespace='first-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96028e-d74a-41e3-b2c7-8ab4117ac524",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[random.random() for _ in range(1536)] for v in range(2)]\n",
    "# print(vectors)\n",
    "ids = list('mn')\n",
    "\n",
    "index.upsert(vectors=zip(ids, vectors), namespace='second-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e2d08-38b2-4432-ab01-e058c97ccc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15765a18-dfd3-4bdb-a6cb-d0cdf4e02185",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.fetch(ids=['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e1adf-8380-486b-a0fe-68ec078e3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.fetch(ids=['x'], namespace='first-namespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f780b4-a644-4fb0-9f01-9d90d36e6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deleting namespace vectors either single or all\n",
    "# delete single or multiple\n",
    "index.delete(ids=['x', 'z'], namespace='first-namespace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efdb38c-9051-4148-b70f-969c873cea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all vectors in namespace\n",
    "index.delete(delete_all=True, namespace='first-namespace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26baad-34bc-4506-82e7-fd7100d826fe",
   "metadata": {},
   "source": [
    "# Splitting and Embedding Text Using Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ee933-93e7-4585-a489-ef9a8990050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open('./churchill_speech.txt') as f:\n",
    "    churchill_speech = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([churchill_speech])\n",
    "# print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b543c2-5325-4ab1-be8b-4c3f6d7aa4e8",
   "metadata": {},
   "source": [
    "# Embedding Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43385b-aaf3-4e75-96e7-b8b3755ef706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total tokens: {total_tokens}')\n",
    "    print(f'Embedding cost is USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e1c53-d17c-4f5f-992c-c5bc2f40b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedding.embed_query(chunks[0].page_content)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787a926-29df-402c-801f-41ee5e6fd4a6",
   "metadata": {},
   "source": [
    "## Inserting the Embeddings into a PineCone Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245052a-46e7-425e-b81b-67cb4de106f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "pc = pinecone.Pinecone()\n",
    "\n",
    "for i in pc.list_indexes().names():\n",
    "    print('Deleting all indexes....', end='')\n",
    "    pc.delete_index(i)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593e3b5-ea7e-4dae-97b1-b0766ca2af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'churchill-speech'\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f'Creating index {index_name} ...')\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=pinecone.PodSpec(\n",
    "            environment='gcp-starter'\n",
    "        )\n",
    "    )\n",
    "    print('Done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735bfb8-e483-4306-b65c-b204324ac904",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00401d-92f7-4179-8c64-3ebc91a9bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Pinecone.from_documents(chunks, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517e6be-a089-40a3-9f10-699ca322501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the vector store from an existing index\n",
    "# vector_store = Pinecone.from_existing_index(index_name=index_name, embedding=embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d6a16-24b2-498e-bef4-bb3c647ec04e",
   "metadata": {},
   "source": [
    "# Asking Questions (Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5742937-483c-4a5b-8251-f31c61befe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Where should we fight?'\n",
    "result = vector_store.similarity_search(query)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcf23f-0f14-40e0-b1bf-3dac03694e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc52a3-c358-46dd-a3bc-12219344d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1583f9c-ef84-446d-b155-5398a14fa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'Where should we fight?'\n",
    "# query = 'Who were the Kings of England and Belgium at the time?' \n",
    "query = 'What about the French Armies??'\n",
    "answer = chain.invoke(query)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c29793-079e-420f-8a92-6bb8881e4e8d",
   "metadata": {},
   "source": [
    "# 20240303 - Gemini\n",
    "\n",
    "- Gemini is a family of hihgly capable multimodal models developed by Google Deepmind.\n",
    "- A multimodal model is an AI model that can process and understand information from multiple sources, such as text, audio and video.\n",
    "- Gemini was trained on a massive dataset of images, audio, video, text and code.\n",
    "- Gemini Model Family:\n",
    "- 1. Gemini Ultra\n",
    "     The largest and most capable model.\n",
    "  3. Gemini Pro\n",
    "     The best model for scaling\n",
    "  5. Gemini Nano\n",
    "     1.8-3.2B Parameters - the most efficient model for on-device (mobile) deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e933c-3d07-4773-89ad-ea8771fb6ee4",
   "metadata": {},
   "source": [
    "Langchain and Google's Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80b093-f27f-45ea-b4a5-0cca35db716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langchain's google gemini libr\n",
    "pip install --upgrade -q langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a40a9a-c7f9-41b0-9f3f-691c382d2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b9155-beb9-48fc-a8b4-eafe7005a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install google's generative ai lib\n",
    "\n",
    "pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1687261-b0a7-467d-a583-3ad189d507ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1e11b-14a3-4398-a3e6-346ef0637381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "if 'GOOGLE_API_KEY' not in os.environ:\n",
    "    os.environ['GOOGLE_API_KEY'] = getpass.getpass('Provide your Google API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da636ab-46e8-49e4-86fe-4dd988ba2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "for model in genai.list_models():\n",
    "    print(model.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca1a10-1ce0-4aa9-a829-39163bdd9e30",
   "metadata": {},
   "source": [
    "# Integrating Gemini with LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3676dba-c498-481c-a0a0-c54200ce5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.9)\n",
    "\n",
    "response = llm.invoke('Write a paragraph about life on Mars in year 2100.')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14eaa29-650e-41dd-ae92-faee46d662ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
    "\n",
    "prompt = PromptTemplate.from_template('You are a content creator. Write me a tweet about {topic}')\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topic = 'Why will AI change the world'\n",
    "response = chain.invoke(input=topic)\n",
    "print(response) # or response['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce3dd6-8752-4ec6-93be-aed36a804295",
   "metadata": {},
   "source": [
    "# System Prompt and Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1769b-8ecf-4b04-ac64-2a166870926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', convert_system_message_to_human=True)\n",
    "output = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content='Answer only YES or NO in French.'),\n",
    "        HumanMessage(content='Is fish a mamal?')\n",
    "    ]\n",
    ")\n",
    "\n",
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e537f6e-ac07-4e20-9bc0-d6a77cd8f485",
   "metadata": {},
   "source": [
    "# Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6ba81-0d36-42d5-9847-a8046c4d0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)\n",
    "prompt = 'Write a scientific paper outlining the mathematical foundation of our universe.'\n",
    "# response = llm.invoke(prompt)\n",
    "# print(response.content)\n",
    "import time\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content)\n",
    "    print('-'*100)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cad5cf-eff1-4b5c-8a5b-a6712d090d0d",
   "metadata": {},
   "source": [
    "# Multimodal AI with Gemini Pro Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c06be-537e-4789-8af1-93992c204c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6a9ee-e376-4fdb-8016-26edd881ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('./random_image.jpeg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31235566-1534-4b78-8431-1b9708564a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from PIL import Image\n",
    "# load env vars\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "img = Image.open('./random_image.jpeg')\n",
    "img\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro-vision')\n",
    "prompt = 'What is in this image?'\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {'type': 'text', 'text': prompt},\n",
    "        {'type': 'image_url', 'image_url': img}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47e84b-e35a-471a-9e1e-d23c0964f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pillow\n",
    "# pip show pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65f754-c11a-4e91-9634-8e5f6b7a576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(text, image, model='gemini-pro-vision'):\n",
    "    llm = ChatGoogleGenerativeAI(model=model)\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {'type': 'text', 'text': prompt},\n",
    "            {'type': 'image_url', 'image_url': img}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([message])\n",
    "    return response\n",
    "\n",
    "response = ask_gemini('What is this image content? How can I identify the sport in this picture?', img)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8a9d4-c3d4-4232-8b91-c4af8a9195aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Image\n",
    "image_url = 'https://picsum.photos/id/40/4106/2806'\n",
    "content = requests.get(image_url).content\n",
    "image_data = Image(content)\n",
    "image_data\n",
    "response2 = ask_gemini('Describe this image as detailed as possible', image_data)\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7994e-e366-4314-b129-dc0df23c5c99",
   "metadata": {},
   "source": [
    "# Jupyter AI\n",
    "# JupyterLab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d801ba8-34fd-4351-bf4d-61aa5f96ad76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
