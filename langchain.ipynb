{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1e39e-6b62-4f2b-bbfa-48a3e38e9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f413dc-9537-4dbc-9825-27592b237f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.')\n",
    "print(output.content)\n",
    "# help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00c968-a218-4cb8-a3d2-0ddff85d8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(message)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3485cb-8625-4ab4-b671-cea09cc70dea",
   "metadata": {},
   "source": [
    "### In-Memory Cache - Caching LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdeac04-de8f-4eb2-98e1-285992b588d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243834b-7dc3-48f9-a93a-ba5871c43ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa5ae5-898f-4560-884b-a7b9eeebf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f082d6-a856-48b0-92af-ad6015355842",
   "metadata": {},
   "source": [
    "### SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374eeb3-b5ff-43e1-9b70-f540df3b98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.sqlite.db\"))\n",
    "\n",
    "prompt2 = \"Tell me a joke\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5f78d-48a9-4840-bf14-6c1ca5853920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# first request (not in cache, takes longer)\n",
    "\n",
    "llm.invoke(prompt2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a8dd5-f1dc-465f-b4cd-d2f3a8bc25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# second request (cached, faster)\n",
    "llm.invoke(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82bbe8-d001-4624-a0d1-1d33bfbaacf1",
   "metadata": {},
   "source": [
    "## LLM Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67d50-e090-47f4-becc-f65340e0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f7cc0-0ecb-421b-b094-6f637d1bc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97ea07-b4cb-415e-9370-bb22da4319d2",
   "metadata": {},
   "source": [
    "# 20240301 - PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404d2d4-f609-458a-aa0c-0ef1987ad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in \"{language}\".'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language='english')\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe85cd-b2a8-4fef-addf-c2a9ff4074c4",
   "metadata": {},
   "source": [
    "# 20240301 - ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad4473-2a00-42ab-bf94-f6541f0e9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population. Show the population in millions.')\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(n='20', area='Africa')\n",
    "print(messages)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa1828-1198-4204-8a3d-17aa0d9ebb9e",
   "metadata": {},
   "source": [
    "20240301 - Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4dbb7-4d87-4b49-beea-50acb6b3c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in \"{language}\".'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'HSV', 'language': 'English'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b634ab8-e152-4b42-a453-d5799e14a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points.'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "output = chain.invoke(country)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aafbb5-0af6-4b35-b254-af73454b6c2b",
   "metadata": {},
   "source": [
    "# 20240301 - Sequential Chains\n",
    "With Sequential chains, you can make a series of calls to one or more LLMs. You can take the output from one chain and use it as the input to another chain.\n",
    "\n",
    "There are two types of sequential chains:\n",
    "1. SimpleSequentialChain\n",
    "2. General form of sequential chains\n",
    "\n",
    "SimpleSequentialChain represents a series of chains, where each individual chain has a single input and a single output, and the output of one step is used as input to the next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b19a1-f5b0-4872-886d-c920d6c4c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "     template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('linear regression')\n",
    "print(output['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658c0d-7ad7-4b6b-b56b-2f916a82a0db",
   "metadata": {},
   "source": [
    "# 20240301 - Agents - LangChain Agents in Action: Python REPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e35b0f-258c-42ee-b25b-dd25bafacd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c1b35-abc9-41c6-a26e-31f5d19fcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1932d-8847-479d-89c6-bffd1fe5d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88111d36-d8a9-4805-8d4c-1a123ef6ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "fact12 = math.factorial(12)\n",
    "sqrtFact12 = math.sqrt(fact12)\n",
    "roundSqrtFact12 = round(sqrtFact12, 4)\n",
    "\n",
    "print(fact12, sqrtFact12, roundSqrtFact12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c942b7-84fa-4349-8359-a921e4c037ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af243dd9-1ee8-4372-87d3-cde8d4cc34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6093efb-e189-44fe-b7cc-b6add506bfb4",
   "metadata": {},
   "source": [
    "# 20240301 - Langchain tools- DuckDuckGo and Wikipedia, Google Search\n",
    "Langchain tools are like specialized apps for yout LLM. They are tiny code modules that allow it to access information and services.\n",
    "\n",
    "These tools connect your LLM to search engines, databases, APIs, and more, expanding its knowledge and capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d6a2a-fe28-4b9a-8c28-8863faf8dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53127174-9735-4034-9e89-dda48843f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "output = search.invoke('Where was Adolf Hitler born?')\n",
    "print(output)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7c8ff-3a18-48c1-8eb9-28a9e698b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='en-GB', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Newport')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c48f8-ca0d-41d1-8121-34cfb5f2f2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87779cc3-0754-4907-82fd-4bd8daebbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = DuckDuckGoSearchResults()\n",
    "output2 = search2.run('Freddie Mercury and Queen')\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906932f9-84d7-4e69-b426-ba861b44d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229f0d-ca56-4b45-8af2-7db8295c3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a9745-0f75-41fd-a7be-b95ac29a210f",
   "metadata": {},
   "source": [
    "# Wikipedia tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1813d-8053-4fcb-b64b-ad2e509cf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a121fcd-0d19-4300-b530-fb915659a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=5000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.invoke({'query': 'llamaindex'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9c0ed-295c-460e-9e5d-4464a4ab01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.invoke('Great Wall of China')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d2528-4f2f-4748-831c-5b04f602c756",
   "metadata": {},
   "source": [
    "# 20240301 - Reasoning and Acting (ReAct)\n",
    "ReAct is a new approach that combines reasoning (chain of thoughts prompting) and acting capabilities of LLMs.\n",
    "\n",
    "With ReAct LLMs generate reasoning traces and task-specific actions in an interleaved manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c684b-c9fe-426e-9d54-8249d03ae8e2",
   "metadata": {},
   "source": [
    "# Creating a ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546c1b9-6fb0-496d-8cad-3746b48ce0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "template = '''\n",
    "Anser the following questions as best as you can. Translate to FRENCH.\n",
    "Questions: {q}\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "# print(type(prompt))\n",
    "# print(prompt.input_variables)\n",
    "# print(prompt.template)\n",
    "# 1. PythonREPL tool\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")\n",
    "\n",
    "# 2. Wikipedia Tool (for searchin Wikipedia)\n",
    "wiki_api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "ddg_search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=ddg_search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89108423-5ef5-4ea4-b8b3-63c19a535046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "# question = 'Who is the current Prime Minister of U.K.?'\n",
    "question = 'Tell me about Makaveli early life'\n",
    "\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541905b9-4302-46f3-ba19-4454ab300f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
