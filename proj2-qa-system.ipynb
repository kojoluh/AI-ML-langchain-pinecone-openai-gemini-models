{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6694e048-1f7c-4b07-af38-f0b503567fa9",
   "metadata": {},
   "source": [
    "# 2024-03-06 \n",
    "# Question-Answering System on private documents using openai, pinecone and langchain - OPL/APL\n",
    "\n",
    "GPT models can answer questions based on training set from earlier. But what if the data is private or untrained earlier?.\n",
    "How can LLMs learn new knowledge? \n",
    "1. Fine-tuning on a training set (expensive and time consuming)\n",
    "2. Model inputs - (ideal and simple, limited by token size max (4000 tokens/chunks) etc)\n",
    "\n",
    "Question-Answering pipeline \n",
    "1. Prepare the document (once per document)\n",
    "   - Load the data into Langchain Documents\n",
    "   - Split the documents into chunks\n",
    "   - Embed the chunks into nuermic vectors\n",
    "   - Save the chunks and the ebeddings to a vector database (Pinecone, Milvus or Quadrant)\n",
    "  \n",
    "2. Search (once per query)\n",
    "   - Embed the user's question\n",
    "   - Using the question's embeddings and the chunk embeddings, rank the vectors by similarity to the question's embedding. The nearest vectors represent the chunks similar to the question.\n",
    "  \n",
    "3. Ask (once per query)\n",
    "   - Insert the question and the most relevant chunks into a message to a GPT model.\n",
    "   - Return GPT's answer.\n",
    "   - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0ed7e8-f6ad-423d-b4a9-41dba8e36fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccd2b4-ad0a-4bcd-a9ad-12a02f8b6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e65ba3-8232-4ba4-a19f-17c15506b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a63f9b-64de-4522-b3fe-c51a091ee9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unstructured -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3836d-f492-4d1d-8f60-600add311e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Loads pdfs using PyPDF and returns array of documents \n",
    "where each doc is content page, page number etc\n",
    "'''\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "    print(f'Loading {file}')\n",
    "\n",
    "    if extension == '.pdf':  \n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "\n",
    "    elif extension =='.md':\n",
    "        # pip install unstructured > /dev/null\n",
    "        from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "        loader = UnstructuredMarkdownLoader(file)\n",
    "\n",
    "    elif extension == '.json':\n",
    "        # pip install jq\n",
    "        from langchain_community.document_loaders import JSONLoader\n",
    "        \n",
    "        import json\n",
    "        from pathlib import Path\n",
    "        from pprint import pprint\n",
    "        loader = JSONLoader(\n",
    "            file_path=file,\n",
    "            jq_schema='[]', # not sure how to parse the json content yet, refer to jq for the jq_schema\n",
    "            text_content=True\n",
    "        )\n",
    "    else:\n",
    "        print(f'Document format {extension} is not supported')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ba811-c912-4f13-9802-4951c923caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b39a70-2826-4237-8f5e-4bf143a444a9",
   "metadata": {},
   "source": [
    "Display data / Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ebb3e-1f00-47e4-9c4b-03ef1ebcbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/ghana_constitution.pdf')\n",
    "print(data[1].page_content)\n",
    "# print(data[10].metadata)\n",
    "# print(len(data))\n",
    "# print(f'There are {len(data[10].page_content)} characters in page 10')\n",
    "# can also url of pdf into function and it will retrieve data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4515ad-9c87-4732-9bd8-f628147b1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_document('files/sample_docx_file.docx')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b7660-79b1-497c-a404-4dc088f8df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_document('files/chat_history.json')\n",
    "# print(data[0]) ## still not working, take some time to adjust jq_schema above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285d515-f979-4f57-ab06-d5137caac9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_document('README.md')\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02780014-88f2-4d18-a552-c373cb9718c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading wikipedia external loaders\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04728d41-1cf4-4d2e-90e4-670b2fce16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_from_wikipedia('GPT-4')\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e75a33-45e3-4322-bc13-5f2f3b626f76",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "- is the process of breaking down large pieces of text into smaller segments.\n",
    "- Its an essential technique that helps optimize the relevance of the content we get back from a vector database.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3d62c-0e4a-4745-9fa4-dc5e5f2585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a3fa1-a577-425d-9c18-8dd6a0ccbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading data from file and chunking all at once\n",
    "# data = load_document('files/ghana_constitution.pdf')\n",
    "# chunks = chunk_data(data)\n",
    "# print(f'We have {len(chunks)} in ghana constitution file of {len(data)} pages')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b32c69-5c09-4a20-af5d-69b73a0bd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83cc6f-53be-4cb7-be1a-9406cd7c1b60",
   "metadata": {},
   "source": [
    "# Check and Print Embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db0d1c-592a-4b53-99e2-3c6c2a3155a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embedding costs\n",
    "def print_embedding_costs(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e7755-4773-4b4e-94a9-9568ac7de0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_embedding_costs(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29203f-f4ed-4fe7-a648-4163aaf16e9e",
   "metadata": {},
   "source": [
    "Embedding and Uploading to a Vector Database (PineCone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b68f7-fb1b-4f42-8ce3-8c4ec3345aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting of embeddings method\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    from pinecone import PodSpec\n",
    "    import pinecone\n",
    "    import os\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    pcone = pinecone.Pinecone()\n",
    "    indexes = pcone.list_indexes().names()\n",
    "    if index_name in indexes:\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embedding)\n",
    "        print('Ok\\n')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pcone.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        print(f'Index_name: {index_name}, Chunks: {len(chunks)}...', end='')\n",
    "        vector_store = Pinecone.from_documents(chunks, embedding, index_name=index_name)\n",
    "        print('Ok\\n')\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877ef98-7dad-4ba9-aacf-aa20a643dd0f",
   "metadata": {},
   "source": [
    "Delete Index Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9c7116-dad0-45a8-9b7d-b7971e426ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index function\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    import os\n",
    "    \n",
    "    pcone = Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pcone.list_indexes().names()\n",
    "        print(indexes)\n",
    "        print('Deleting all indexes...')\n",
    "        for index in indexes:\n",
    "            pcone.delete_index(index)\n",
    "        print('Ok\\n')\n",
    "    else:\n",
    "        print(f'\\nDeleting index {index_name} ...', end='')\n",
    "        pcone.delete_index(index_name)\n",
    "        print('Ok\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1eef61-9a94-4b69-88ab-7fed912ded0a",
   "metadata": {},
   "source": [
    "Ask and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca539a12-735b-4b6c-b52a-b8e87e94f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask and get answer function\n",
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eede23b-7260-446d-a220-78c887950be0",
   "metadata": {},
   "source": [
    "Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166b10e1-0861-46cb-bf06-6aee142d2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Deleting all indexes...\n",
      "Ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484dffb5-2ed6-444f-9cb1-ece65a880dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pincecone_usage_q-n_a():\n",
    "def test_pinecone_usage_q_n_a():\n",
    "    # creating index and storing data in vectorstore\n",
    "    index_name = 'askadocument'\n",
    "    data = load_document('files/ghana_constitution.pdf')\n",
    "    chunks = chunk_data(data)\n",
    "    vector_store = insert_or_fetch_embeddings(index_name, chunks)\n",
    "    #quering the vector store\n",
    "    \n",
    "    query = 'What is the whole document about?'\n",
    "    answer = ask_and_get_answer(vector_store, query)\n",
    "    print(answer)\n",
    "    query = 'What is the legal age requirements of President of Ghana?'\n",
    "    answer = ask_and_get_answer(vector_store, query)\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554ea13-8247-4ce4-a475-86374bb1e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pinecone_usage_q_n_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eaccbc-3831-4acc-a24a-a6b639d44c16",
   "metadata": {},
   "source": [
    "# Continuous Questions until quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959c063-0968-49b3-b7b4-78221e29e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_until_quit():\n",
    "    import time\n",
    "    i = 1\n",
    "    print('Write/input Quit or Exit to quit.')\n",
    "    while True:\n",
    "        q = input(f'Question #{i}: ')\n",
    "        i += 1\n",
    "        if q.lower() in ['quit', 'exit']:\n",
    "            print('Quiting ... bye bye!')\n",
    "            time.sleep(2)\n",
    "            break\n",
    "        answer = ask_and_get_answer(vector_store, q)\n",
    "        print(f'\\nAnswer: {answer}')\n",
    "        print(f'\\n {\"-\" * 50} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb29b4c-95b1-4288-a530-3ca4b333feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask_question_until_quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818e3f5-d654-4205-a5c8-138bc008b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki_data(topic):\n",
    "    index_name = 'wikipedia'\n",
    "    delete_pinecone_index()\n",
    "    \n",
    "    data = load_from_wikipedia(topic, 'en')\n",
    "    chunks = chunk_data(data)\n",
    "    print(f'\\nData: {len(data)}, Chunks: {len(chunks)}\\n', end='')\n",
    "    \n",
    "    vector_store = insert_or_fetch_embeddings(index_name, chunks)\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7bd38-c7bd-4112-9924-8328eec61f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask wikipedia a question\n",
    "# topic = 'French History'\n",
    "# vector_store = load_wiki_data(topic)\n",
    "# q = f'What is {topic}?'\n",
    "# answer = ask_and_get_answer(vector_store, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5090f5-f582-49ad-b30f-16fe62f2e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711b501-49ca-4f5d-b986-275e4c753507",
   "metadata": {},
   "source": [
    "# RAG - Retrieval Augmented Generation\n",
    "- helps overcome knowledge limits, makes answers more factual, and lets the model handle complex questions.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0555dc6-c202-45ad-9916-7442680bf2e4",
   "metadata": {},
   "source": [
    "# Using Chroma as a Vector DB\n",
    "- install using pip install -q chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfe0ca-a097-4691-96f1-5b7ed91420d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06467c-9ad6-455f-83e8-c9f9b81a88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Embeddings - Chroma\n",
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma.from_documents(chunks, embedding, persist_directory=persist_directory)\n",
    "    return vector_store\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82be611-63d6-4fdd-baa2-b74264822b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Embeddings - Chroma\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embedding = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "    return vector_store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b5645-36e7-41ac-9fad-626175280450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing chroma usage\n",
    "def test_chroma_usage():\n",
    "    data = load_document('files/ghana_constitution.pdf')\n",
    "    chunks = chunk_data(data)\n",
    "    # vector_store = create_embeddings_chroma(chunks)\n",
    "    # query = 'What is the whole document about?'\n",
    "    # answer = ask_and_get_answer(vector_store, query)\n",
    "    # print(answer)\n",
    "    # query = 'What is the legal age requirements of President of Ghana?'\n",
    "    # answer = ask_and_get_answer(vector_store, query)\n",
    "    # print(answer)\n",
    "\n",
    "    vector_store = load_embeddings_chroma()\n",
    "    query = 'Who are the next inline to govern the country after the Vice-President?'\n",
    "    answer = ask_and_get_answer(vector_store, query)\n",
    "    print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16580c-4b49-4c16-9978-0bb2c39d63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69e65f-a843-4a05-b2a8-926baf26464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_chroma_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5160bb3-a0d0-4f3a-8688-d704ce9f7422",
   "metadata": {},
   "source": [
    "# Memory- Adding Memory (Chat history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac064b-7f93-441a-b3f2-1af2d330ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory_to_chat():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        chain_type='stuff',\n",
    "        verbose=True\n",
    "    )\n",
    "    return crc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb644fa7-706d-4b1d-a802-3799f15c2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_memorized_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc7af-4c21-4865-baa8-5c34c8f973ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chroma_memorized_usage():\n",
    "    data = load_document('files/ghana_constitution.pdf')\n",
    "    chunks = chunk_data(data)\n",
    "    # vector_store = create_embeddings_chroma(chunks)\n",
    "    # query = 'What is the whole document about?'\n",
    "    # answer = ask_and_get_answer(vector_store, query)\n",
    "    # print(answer)\n",
    "    # query = 'What is the legal age requirements of President of Ghana?'\n",
    "    # answer = ask_and_get_answer(vector_store, query)\n",
    "    # print(answer)\n",
    "    chain = add_memory_to_chat()\n",
    "    vector_store = load_embeddings_chroma()\n",
    "    query = 'Who are the next inline to govern the country after the Vice-President?'\n",
    "    # answer = ask_and_get_answer(vector_store, query)\n",
    "    answer = ask_memorized_question(query, chain)\n",
    "    print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c30c2b-ba57-403e-89e6-a8abc4072e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_chroma_memorized_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f91583-922e-4fa6-b0fc-7d3d3f0eade1",
   "metadata": {},
   "source": [
    "# 20240307\n",
    "# Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424777a-ba53-4ae9-991a-e21162948cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_custom_prompts():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5.-turbo', temperature=0)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "    memory = ConversationBufferMemory(memoryKey='chat_history', return_message=True)\n",
    "\n",
    "    system_template = r'''\n",
    "    User the following pieces of context to answer the user's question.\n",
    "    --------------------\n",
    "    Context: ```{context}```\n",
    "\n",
    "    '''\n",
    "\n",
    "    user_template = '''\n",
    "    Question: ```{question}```\n",
    "    Chat History: ```{chat_history}```\n",
    "    '''\n",
    "\n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        HumanMessagePromptTemplate.from_template(user_template)\n",
    "    ]\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        chain_type='stuff',\n",
    "        combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "        verbose=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2407e3-f532-4255-b194-edf4b00ee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers has the StackOverfolw dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a94e3-33ce-4b70-8554-e50b20a0bd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
